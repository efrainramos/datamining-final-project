col = rainbow(10),
ylab = 'count',
main = 'Sentiment')
return(barplot)
}
get_sentiment_barplot(sentiment_analysis[[1]])
spotify_playlists_data <- clean_list
sentiment_analysis <- list()
for(i in 1:length(spotify_playlists_data)) {
sentiment_analysis[[i]] <- get_nrc_sentiment(spotify_playlists_data[[i]]$lyrics)
}
get_sentiment_barplot <- function(sentiment_analysis_data) {
barplot <- barplot(colSums(sentiment_analysis_data),
las = 2,
col = rainbow(10),
ylab = 'count',
main = 'Sentiment')
return(barplot)
}
get_sentiment_barplot(sentiment_analysis[[1]])
clean_list[[1]]$lyrics
MrClean <- function(list){
list_avgs <- lapply(list, function(x) lapply(x, mean, na.rm=TRUE))
print(list_avgs)
str(list_avgs)
for (i in 1:length(list)) {
print(i)
# Set the text to lowercase
list[[i]]$lyrics <- tolower(list[[i]]$lyrics)
# Remove mentions, urls, emojis, numbers, punctuations, etc.
list[[i]]$lyrics <- gsub("@\\w+", "", list[[i]]$lyrics)
list[[i]]$lyrics <- gsub("https?://.+", "", list[[i]]$lyrics)
list[[i]]$lyrics <- gsub("\\d+\\w*\\d*", "", list[[i]]$lyrics)
list[[i]]$lyrics <- gsub("#\\w+", "", list[[i]]$lyrics)
list[[i]]$lyrics <- gsub("[^\x01-\x7F]", "", list[[i]]$lyrics)
list[[i]]$lyrics <- gsub("[[:punct:]]", "", list[[i]]$lyrics)
# Remove spaces and newlines
list[[i]]$lyrics <- gsub("\n", " ", list[[i]]$lyrics)
list[[i]]$lyrics <- gsub("^\\s+", "", list[[i]]$lyrics)
list[[i]]$lyrics <- gsub("\\s+$", "", list[[i]]$lyrics)
list[[i]]$lyrics <- gsub("[ |\t]+", " ", list[[i]]$lyrics)
list[[i]]$popularity <- replace_na(list[[i]]$popularity, as.integer(list_avgs[[i]]$popularity))
list[[i]]$danceability <- replace_na(list[[i]]$danceability, list_avgs[[i]]$danceability)
list[[i]]$key <- replace_na(list[[i]]$key, as.integer(list_avgs[[i]]$key))
list[[i]]$energy <- replace_na(list[[i]]$energy, list_avgs[[i]]$energy)
list[[i]]$loudness <- replace_na(list[[i]]$loudness, list_avgs[[i]]$loudness)
list[[i]]$mode <- replace_na(list[[i]]$mode, as.integer(list_avgs[[i]]$mode))
list[[i]]$speechiness <- replace_na(list[[i]]$speechiness, list_avgs[[i]]$speechiness)
list[[i]]$acousticness <- replace_na(list[[i]]$acousticness, list_avgs[[i]]$acousticness)
list[[i]]$instrumentalness <- replace_na(list[[i]]$instrumentalness, list_avgs[[i]]$instrumentalness)
list[[i]]$liveness  <- replace_na(list[[i]]$liveness, list_avgs[[i]]$liveness)
list[[i]]$valence <- replace_na(list[[i]]$valence, list_avgs[[i]]$valence)
list[[i]]$tempo <- replace_na(list[[i]]$tempo, list_avgs[[i]]$tempo)
}
return(list)
}
clean_list <- MrClean(list4)
clean_list[[1]]$lyrics
testLyrics <- list1[[1]]$lyrics
testLyrics
s <- get_nrc_sentiment(testLyrics)
s
barplot(colSums(s),
las = 2,
col = rainbow(10),
ylab = 'count',
main = 'Sentiment')
testLyrics
list1[[1]]$lyrics
View(list1)
View(testLyrics)
View(list4[[1]])
df <- View(list4[[1]])
df <- list4[[1]]
df$lyrics
View(df)
df$lyrics[1]
tokens <- df %>%
unnest_tokens(input = "lyrics",
output = "text") %>%
group_by(word) %>%
tally(name = "freq") %>%
arrange(desc(freq))
tokens <- df %>%
unnest_tokens(input = "lyrics",
output = "text") %>%
group_by(lyrics) %>%
tally(name = "freq") %>%
arrange(desc(freq))
colnames(df)
tokens <- df %>%
unnest_tokens(input = "lyrics",
output = "text") %>%
group_by(text) %>%
tally(name = "freq") %>%
arrange(desc(freq))
View(tokens)
tokens <- df[1] %>%
unnest_tokens(input = "lyrics",
output = "text") %>%
group_by(text) %>%
tally(name = "freq") %>%
arrange(desc(freq))
tokens <- df[1, ] %>%
unnest_tokens(input = "lyrics",
output = "text") %>%
group_by(text) %>%
tally(name = "freq") %>%
arrange(desc(freq))
tokens
MrClean_Token <- function(list){
tokens <- list %>%
unnest_tokens(input = "lyrics",
output = "text") %>%
group_by(text) %>%
tally(name = "freq") %>%
arrange(desc(freq))
}
token1 <- MrClean_Token(clean_list[[1]])
View(token1)
df <- list4[[1]]
df$lyrics[1]
clean_list <- MrClean(list4)
df <- clean_list[[1]]
df$lyrics[1]
colnames(df)
# This is one playlist
tokens <- df %>%
unnest_tokens(input = "lyrics",
output = "text") %>%
group_by(text) %>%
tally(name = "freq") %>%
arrange(desc(freq))
View(tokens)
# Remove stop words
tokens <- tokens %>%
anti_join(get_stopwords(), by = "word")
library(spotifyr)
library(genius)
library(tm)
library(wordcloud)
library(XML)
library(tidyverse)
library(tidytext)
library(syuzhet)
library(rtweet)         # For parsing the JSON
library(textclean)      # For removing contractions
library(stringr)        # For data cleaning
library(dplyr)          # For data manipulation
library(ggplot2)        # For plotting graphs
library(reshape2)       # For melting data
library(ggmap)
library(corrplot)
library(RColorBrewer)
library(textdata)
# Remove stop words
tokens <- tokens %>%
anti_join(get_stopwords(), by = "word")
nrc_sent <- tokens %>%
select(word) %>%
inner_join(get_sentiments("nrc")) %>%
count(word, sentiment) %>%
arrange(desc(n))
nrc_sent <- tokens %>%
select("word") %>%
inner_join(get_sentiments("nrc")) %>%
count(word, sentiment) %>%
arrange(desc(n))
nrc_sent <- tokens %>%
select(text) %>%
inner_join(get_sentiments("nrc")) %>%
count(text, sentiment) %>%
arrange(desc(n))
MrClean_Token <- function(list){
tokens <- list %>%
unnest_tokens(input = "lyrics",
output = "text") %>%
group_by(text) %>%
tally(name = "freq") %>%
arrange(desc(freq))
}
token1 <- MrClean_Token(clean_list[[1]])
token1 <- MrClean_Token(clean_list[[1]])
token1 <- MrClean_Token(df)
# Remove stop words
token1 <- token1 %>%
anti_join(get_stopwords("en"), by = "text")
?get_stopwords
library(dplyr)
# Remove stop words
token1 <- token1 %>%
anti_join(get_stopwords("en"), by = "text")
nrc_sent <- token1 %>%
select(text) %>%
inner_join(get_sentiments("nrc")) %>%
count(text, sentiment) %>%
arrange(desc(n))
nrc_sent <- token1 %>%
select(text) %>%
inner_join(get_sentiments("nrc"), by = "text") %>%
count(text, sentiment) %>%
arrange(desc(n))
get_sentiments("nrc")
nrc_sent <- token1 %>%
select(text) %>%
inner_join(get_sentiments("nrc"), by = text) %>%
count(text, sentiment) %>%
arrange(desc(n))
nrc_sent <- token1 %>%
select(text) %>%
inner_join(get_sentiments("nrc"), by = token1$text) %>%
count(text, sentiment) %>%
arrange(desc(n))
nrc_sent <- token1 %>%
select(text) %>%
inner_join(get_sentiments("nrc"), by = text) %>%
count(text, sentiment) %>%
arrange(desc(n))
MrClean_Token <- function(list){
tokens <- list %>%
unnest_tokens(input = "lyrics",
output = "word") %>%
group_by(text) %>%
tally(name = "freq") %>%
arrange(desc(freq))
}
token1 <- MrClean_Token(clean_list[[1]])
MrClean_Token <- function(list){
tokens <- list %>%
unnest_tokens(input = "lyrics",
output = "text") %>%
group_by(text) %>%
tally(name = "freq") %>%
arrange(desc(freq))
}
token1 <- MrClean_Token(clean_list[[1]])
nrc_sent <- token1 %>%
select(text) %>%
inner_join(get_sentiments("nrc"), by = word) %>%
count(text, sentiment) %>%
arrange(desc(n))
MrClean_Token <- function(list){
tokens <- list %>%
unnest_tokens(input = "lyrics",
output = "text") %>%
group_by(text) %>%
tally(name = "freq") %>%
arrange(desc(freq))
}
token1 <- MrClean_Token(clean_list[[1]])
nrc_sent <- token1 %>%
select("text") %>%
inner_join(get_sentiments("nrc"), by = word) %>%
count(text, sentiment) %>%
arrange(desc(n))
library(stopwords)
MrClean_Token <- function(list){
tokens <- list %>%
unnest_tokens(input = "lyrics",
output = "word") %>%
group_by(word) %>%
tally(name = "freq") %>%
arrange(desc(freq))
}
token1 <- MrClean_Token(clean_list[[1]])
token1 <- MrClean_Token(clean_list[[1]])
nrc_sent <- token1 %>%
select(word) %>%
inner_join(get_sentiments("nrc")) %>%
count(word, sentiment) %>%
arrange(desc(n))
nrc_sent
View(nrc_sent)
nrc_polarity <- nrc_sent %>%
filter(sentiment == "negative" | sentiment == "positive") %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup %>%
mutate(word = reorder(word, n))
nrc_polarity
ggplot(nrc_polarity, aes(x = word, y = n, fill = sentiment)) +
geom_col() +
facet_wrap(~sentiment, scales = "free_y") +
labs(title = "Sentiments in One Hour of Gamestop Tweets",
x = NULL,
y = "Number of Words") +
coord_flip()
token1 <- MrClean_Token(clean_list[[1]])
token1 <- token1 %>%
anti_join(get_stopwords(), by = "word")
nrc_sent <- token1 %>%
select(word) %>%
inner_join(get_sentiments("nrc")) %>%
count(word, sentiment) %>%
arrange(desc(n))
nrc_polarity <- nrc_sent %>%
filter(sentiment == "negative" | sentiment == "positive") %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup %>%
mutate(word = reorder(word, n))
ggplot(nrc_polarity, aes(x = word, y = n, fill = sentiment)) +
geom_col() +
facet_wrap(~sentiment, scales = "free_y") +
labs(title = "Sentiments in One Hour of Gamestop Tweets",
x = NULL,
y = "Number of Words") +
coord_flip()
View(nrc_polarity)
token1 <- MrClean_Token(clean_list[[1]])
View(token1)
# Remove stop words
token1 <- token1 %>%
anti_join(get_stopwords(), by = "word")
nrc_sent <- token1 %>%
select(word) %>%
inner_join(get_sentiments("nrc"))
nrc_sent <- token1 %>%
select(word) %>%
inner_join(get_sentiments("nrc")) %>%
count(word, sentiment
nrc_sent <- token1 %>%
select(word) %>%
inner_join(get_sentiments("nrc")) %>%
count(word, sentiment)
nrc_sent <- token1 %>%
select(word) %>%
inner_join(get_sentiments("nrc")) %>%
count(word, sentiment) %>%
arrange(desc(n))
token1 <- MrClean_Token(clean_list[[1]])
# Remove stop words
token1 <- token1 %>%
anti_join(get_stopwords(), by = "word")
token1 <- MrClean_Token(clean_list[[1]])
# Remove stop words
token1 <- token1 %>%
anti_join(get_stopwords(), by = "word")
nrc_sent <- token1 %>%
select(word) %>%
inner_join(get_sentiments("nrc")) %>%
count(word, sentiment) %>%
arrange(desc(n))
nrc_sent
View(nrc_sent)
# Remove stop words
token1 <- token1 %>%
anti_join(get_stopwords(), by = "word")
nrc_sent <- token1 %>%
select(word) %>%
inner_join(get_sentiments("nrc")) %>%
count(word, sentiment) %>%
arrange(desc(n))
nrc_polarity <- nrc_sent %>%
filter(sentiment == "negative" | sentiment == "positive") %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup %>%
mutate(word = reorder(word, n))
ggplot(nrc_polarity, aes(x = word, y = n, fill = sentiment)) +
geom_col() +
facet_wrap(~sentiment, scales = "free_y") +
labs(title = "Sentiments in One Hour of Gamestop Tweets",
x = NULL,
y = "Number of Words") +
coord_flip()
token1 <- MrClean_Token(clean_list[[1]])
# Remove stop words
token1 <- token1 %>%
anti_join(get_stopwords(), by = "word")
df1 <- clean_list[[1]]
df2 <- clean_list[[2]]
df3 <- clean_list[[3]]
token1 <- MrClean_Token(Hiphop)
Hiphop <- rbind(df1, df2, df3)
token1 <- MrClean_Token(Hiphop)
# Remove stop words
token1 <- token1 %>%
anti_join(get_stopwords(), by = "word")
nrc_sent <- token1 %>%
select(word) %>%
inner_join(get_sentiments("nrc")) %>%
count(word, sentiment) %>%
arrange(desc(n))
View(clean_list[[1]])
MrClean_Token2 <- function(list){
token2 <- list %>%
select(lyrics) %>%
unnest_tokens("word", "lyrics") %>%
anti_join(get_stopwords())
}
token2 <- MrClean_Token2(clean_list[[1]])
View(token2)
# Remove stop words
token2 <- token2 %>%
anti_join(get_stopwords(), by = "word")
nrc_sent <- token2 %>%
select(word) %>%
inner_join(get_sentiments("nrc")) %>%
count(word, sentiment) %>%
arrange(desc(n))
nrc_polarity <- nrc_sent %>%
filter(sentiment == "negative" | sentiment == "positive") %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup %>%
mutate(word = reorder(word, n))
ggplot(nrc_polarity, aes(x = word, y = n, fill = sentiment)) +
geom_col() +
facet_wrap(~sentiment, scales = "free_y") +
labs(title = "Sentiments in One Hour of Gamestop Tweets",
x = NULL,
y = "Number of Words") +
coord_flip()
ggplot(nrc_sent, aes(x=sentiment, y=n)) +
geom_col()
MrClean_Token <- function(list){
token1 <- list %>%
select(lyrics) %>%
unnest_tokens("word", "lyrics") %>%
anti_join(get_stopwords())
}
token1 <- MrClean_Token(clean_list[[1]])
token1 <- MrClean_Token2(clean_list[[1]])
token1 <- MrClean_Token(clean_list[[1]])
# Remove stop words
token1 <- token1 %>%
anti_join(get_stopwords(), by = "word")
nrc_sent <- token1 %>%
select(word) %>%
inner_join(get_sentiments("nrc")) %>%
count(word, sentiment) %>%
arrange(desc(n))
nrc_polarity <- nrc_sent %>%
filter(sentiment == "negative" | sentiment == "positive") %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup %>%
mutate(word = reorder(word, n))
ggplot(nrc_polarity, aes(x = word, y = n, fill = sentiment)) +
geom_col() +
facet_wrap(~sentiment, scales = "free_y") +
labs(title = "Sentiments in One Hour of Gamestop Tweets",
x = NULL,
y = "Number of Words") +
coord_flip()
ggplot(nrc_sent, aes(x=sentiment, y=n)) +
geom_col()
token1 <- MrClean_Token(clean_list[[1]])
# Remove stop words
token1 <- token1 %>%
anti_join(get_stopwords(), by = "word")
nrc_sent <- token1 %>%
select(word) %>%
inner_join(get_sentiments("nrc")) %>%
count(word, sentiment) %>%
arrange(desc(n))
nrc_polarity <- nrc_sent %>%
filter(sentiment == "negative" | sentiment == "positive") %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup %>%
mutate(word = reorder(word, n))
ggplot(nrc_polarity, aes(x = word, y = n, fill = sentiment)) +
geom_col() +
facet_wrap(~sentiment, scales = "free_y") +
labs(title = "Sentiments in One Hour of Gamestop Tweets",
x = NULL,
y = "Number of Words") +
coord_flip()
ggplot(nrc_sent, aes(x=sentiment, y=n)) +
geom_col()
token1 <- MrClean_Token(Hiphop)
# Remove stop words
token1 <- token1 %>%
anti_join(get_stopwords(), by = "word")
nrc_sent <- token1 %>%
select(word) %>%
inner_join(get_sentiments("nrc")) %>%
count(word, sentiment) %>%
arrange(desc(n))
nrc_polarity <- nrc_sent %>%
filter(sentiment == "negative" | sentiment == "positive") %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup %>%
mutate(word = reorder(word, n))
ggplot(nrc_polarity, aes(x = word, y = n, fill = sentiment)) +
geom_col() +
facet_wrap(~sentiment, scales = "free_y") +
labs(title = "Sentiments in One Hour of Gamestop Tweets",
x = NULL,
y = "Number of Words") +
coord_flip()
ggplot(nrc_sent, aes(x=sentiment, y=n)) +
geom_col()
View(token1)
get_sentiment_data <- function(list){
token <- MrClean_Token(list)
token <- token %>%
anti_join(get_stopwords(), by = "word")
nrc_sent <- token %>%
select(word) %>%
inner_join(get_sentiments("nrc")) %>%
count(word, sentiment) %>%
arrange(desc(n))
return(nrc_sent)
}
playlist1_sentiment <- get_sentiment_data(clean_list[[1]])
View(playlist1_sentiment)
ggplot(playlist1_sentiment, aes(x=sentiment, y=n)) +
geom_col()
